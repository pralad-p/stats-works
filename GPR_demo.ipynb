{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPR_demo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPoFJFWxgeI6yoay4CUD2p8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bhSbcr2bqwAR","executionInfo":{"status":"ok","timestamp":1629884215254,"user_tz":-120,"elapsed":1161,"user":{"displayName":"Pralad Prasad","photoUrl":"","userId":"17967430415026919340"}}},"source":["import sklearn.gaussian_process as gp"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYUt0WBEq1Cj","executionInfo":{"status":"ok","timestamp":1629884256057,"user_tz":-120,"elapsed":347,"user":{"displayName":"Pralad Prasad","photoUrl":"","userId":"17967430415026919340"}}},"source":["# X_tr <-- training observations [# points, # features]\n","# y_tr <-- training labels [# points]\n","# X_te <-- test observations [# points, # features]\n","# y_te <-- test labels [# points]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"tAuYRXLZrBYE","executionInfo":{"status":"ok","timestamp":1629884400266,"user_tz":-120,"elapsed":314,"user":{"displayName":"Pralad Prasad","photoUrl":"","userId":"17967430415026919340"}}},"source":["# The form of the mean function and covariance kernel function in the GP prior is chosen and tuned during model selection. \n","# The mean function is typically constant, either zero or the mean of the training dataset.\n","# The kernel has two hyperparameters: signal variance, σ², and lengthscale, l.\n","kernel = gp.kernels.ConstantKernel(1.0, (1e-1, 1e3)) * gp.kernels.RBF(10.0, (1e-3, 1e3))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFT8-gDbrkmX"},"source":["model = gp.GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1, normalize_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTwDIHhcsF9a"},"source":["# A popular approach to tune the hyperparameters of the covariance kernel function is to maximize the log marginal likelihood of the training data.\n","# A gradient-based optimizer is typically used for efficiency; if unspecified above, the default optimizer is ‘fmin_l_bfgs_b’. \n","# Because the log marginal likelihood is not necessarily convex, multiple restarts of the optimizer with different \n","# initializations is used (n_restarts_optimizer)\n","model.fit(X_tr, y_tr)\n","params = model.kernel_.get_params()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPCwtOoJsTui"},"source":["y_pred, std = model.predict(X_te, return_std=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PeMCqRqMseFG"},"source":["MSE = ((y_pred-y_te)**2).mean()"],"execution_count":null,"outputs":[]}]}